---
layout: blog
root: ../../..
author: Greg Wilson
title: Good Enough Practices in Scientific Computing
date: 2015-04-12
time: "20:00:00"
category: ["Opinion"]
---
<!-- start excerpt -->
<p>
  April Wright recently wrote
  <a href="http://wrightaprilm.github.io/posts/parity-paper.html">a blog post</a>
  about the reproducibility of a paper she recently submitted.
  In it,
  she said:
</p>
<!-- end excerpt -->
<blockquote>
  <p>
    A few people have pointed out recently that open science has kind of an image problem.
    And I fully agree.
    From reading Twitter about open science,
    you'd get the impression that if you don't use the latest tools and your pipeline isn't perfect,
    and you can't just type 'make' and have the full paper spit back at you from the command line,
    then you aren't doing it right...
    Not true, and that's a really alienating message.
  </p>
</blockquote>
<p>
  The root of the problem,
  I think,
  is that while practitioners' ability is distributed on a bell curve,
  public attention is concentrated at the high end.
  As a result,
  even most above-average practitioners think that they're under-performing,
  which is pretty demoralizing:
</p>
<div align="center">
  <img src="impostor-gap.png" class="img-responsive" alt="The Impostor Gap" />
</div>
<p>
  I believe that
  the "impostor gap" between the actual distribution of people's knowledge
  and their (skewed) perception of that distribution
  discourages people who would benefit from our workshops from taking part.
  The fix,
  if we can ever find time to do it,
  would be to write a companion piece to our
  <a href="http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745">best practices paper</a>
  called "Good Enough Practices in Scientific Computing".
  Until then,
  all we can do is ask our instructors to keep telling learners
  the same thing April says in her post:
  "your project doesn't need to be perfectly, fully reproducible to be <em>useful</em>".
</p>
